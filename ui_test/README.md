# Human-in-the-Loop UI Test

A simple web interface for testing the human-in-the-loop functionality of the Intelligent Chat Service.

## Features

- Test chat API with real-time streaming responses
- Handle human-in-the-loop interaction requests
- Submit human feedback responses
- Monitor events with a debug console

## How to Use

1. **Start the Server**: Make sure your intelligent-chat-service API is running.

2. **Launch the UI**: Open `index.html` in a web browser. You can use a simple HTTP server:

   ```
   cd ui_test
   python -m http.server 8080
   ```

   Then open `http://localhost:8080` in your browser.

3. **Configure the API URL**: Enter your API URL (default is `http://localhost:8000`)

4. **Enable Human-in-the-Loop**: Make sure the "Enable Human-in-the-Loop" checkbox is checked.

5. **Start a Chat**: Type a message that might trigger human judgment (see example prompts).

6. **Respond to Human Input Requests**: When the AI needs human input, a panel will appear at the bottom of the chat where you can provide your response.

## Sample Questions to Test Human-in-the-Loop

These questions are specifically designed to trigger the human-in-the-loop functionality:

1. **Ethical dilemmas**:

   - "What's the right balance between AI innovation speed and safety measures?"
   - "Should autonomous systems be allowed to make life-critical decisions without human oversight?"
   - "Is it ethical to deploy AI systems that might displace human workers?"

2. **Opinion requests**:

   - "What's your personal opinion on AI regulation approaches?"
   - "Do you think generative AI will have a net positive or negative effect on society?"
   - "How would you personally balance privacy concerns against AI development needs?"

3. **Complex value judgments**:
   - "Should we prioritize the benefits of new AI technology over potential risks?"
   - "What values should be encoded in AI systems when they face moral dilemmas?"
   - "Who should be responsible when autonomous systems cause harm?"

## Sample Human Responses

When prompted for human input, here are examples of thoughtful responses you might provide:

1. **On balancing innovation and safety**:

   ```
   Safety should be the foundation of AI development, not an afterthought. I recommend a
   tiered approach where higher-risk applications require more rigorous safety testing
   and oversight. For applications with significant potential harm, independent auditing
   and gradual deployment with continuous monitoring are essential. This approach allows
   continued innovation while acknowledging our responsibility to deploy AI safely.
   ```

2. **On AI regulation**:

   ```
   Effective AI regulation requires a combination of industry standards, government oversight,
   and international cooperation. Regulations should focus on high-risk applications while
   allowing more flexibility for lower-risk uses. Transparency requirements, mandatory impact
   assessments, and accountability mechanisms are crucial components. The regulatory framework
   should evolve alongside the technology rather than trying to create a static set of rules.
   ```

3. **On AI ethics and values**:
   ```
   AI systems should be designed with human well-being as their central value, along with
   fairness, transparency, privacy, and respect for human autonomy. When ethical dilemmas
   arise, systems should prioritize harm prevention and have clear escalation paths to
   human decision-makers. Diverse perspectives must be included in defining these values
   to avoid embedding biases or cultural assumptions into supposedly universal principles.
   ```

## Testing Tips

- Use example ethical questions to trigger human-in-the-loop functionality
- Check the event log to see all events generated by the API
- If no human input panel appears, check the server logs to see if the marker was detected
- You can also check for pending interactions by calling the `/ai/human/pending` endpoint directly
- Look for the "HUMAN_HELP_NEEDED:" marker in the content chunks
- Test with varying levels of ethical complexity to see when the system decides to ask for help

## Expected Flow

1. User submits a question with ethical implications
2. Assistant recognizes need for human judgment
3. Assistant includes "HUMAN_HELP_NEEDED:" in response
4. System sends "ui:human:input_requested" event
5. UI shows human input panel
6. Human submits response
7. System sends "ui:human:input_received" event
8. Assistant incorporates human input and updates response
9. System sends "updated_with_human_input" status

## Troubleshooting

- **Connection Issues**: Verify the API URL and ensure CORS is properly configured on your server
- **No Human Input Requests**: Ensure `HUMAN_IN_THE_LOOP_ENABLED=true` is set in your environment
- **UI Not Updating**: Check the browser console for JavaScript errors
- **No Events Appearing**: Verify that the SSE stream is being properly processed
- **Response Not Updated**: Check if the system received the human input correctly via the event log
